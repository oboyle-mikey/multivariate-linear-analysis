PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary = myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq <= 15 & term.freq >=4)
dfFreq <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(dfFreq, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
m <- as.matrix(term.freq)
word.freq <- sort(rowSums(m), decreasing = T)
#word.freq <- subset(word.freq, word.freq <= 30 && word.freq >=10)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F)
library(twitteR)
api_key <- "Av24SqMdj0fi3CWaUG96qWFMu"
api_secret <- "3N2SkIWrUbqlaYqdJt4zxt3h3VStzPGp8WUO0sifx4db8LYxg7"
token <- "593955057-V5KxB4etKztnnL4W07wEj59Zhl98GOZuAwtOQlDl"
token_secret <- "qEEDFUBAgknL9oNnQNWJ6wX0ltudumThuBcbmpP6c0K8h"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
tweets <- userTimeline("Cement Roadstone", n = 100)
tweets.df <- twListToDF(tweets)
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPun <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPun))
#Add Personal Stopwords dependent on search topic
myStopwords <- c(stopwords('english'), "0", "character", "list", "contentauthor", "origin", "datetimestamp", "language", "min", "meta", "heading")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in c(1:2, 98)){
cat(paste0("[", i, "]"))
writeLines(strwrap(as.character(myCorpus[[i]]), 60))
}
stemCompletion2 <- function(x, dictionary){
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary = dictionary)
x <- paste(x, sep="", collapse = " ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary = myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq <= 15 & term.freq >=4)
dfFreq <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(dfFreq, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
m <- as.matrix(term.freq)
word.freq <- sort(rowSums(m), decreasing = T)
#word.freq <- subset(word.freq, word.freq <= 30 && word.freq >=10)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F)
View(tweets.df)
View(tweets.df)
library(twitteR)
api_key <- "Av24SqMdj0fi3CWaUG96qWFMu"
api_secret <- "3N2SkIWrUbqlaYqdJt4zxt3h3VStzPGp8WUO0sifx4db8LYxg7"
token <- "593955057-V5KxB4etKztnnL4W07wEj59Zhl98GOZuAwtOQlDl"
token_secret <- "qEEDFUBAgknL9oNnQNWJ6wX0ltudumThuBcbmpP6c0K8h"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
tweets <- userTimeline("Pwc Ireland", n = 100)
tweets.df <- twListToDF(tweets)
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPun <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPun))
#Add Personal Stopwords dependent on search topic
myStopwords <- c(stopwords('english'), "0", "character", "list", "contentauthor", "origin", "datetimestamp", "language", "min", "meta", "heading")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in c(1:2, 98)){
cat(paste0("[", i, "]"))
writeLines(strwrap(as.character(myCorpus[[i]]), 60))
}
stemCompletion2 <- function(x, dictionary){
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary = dictionary)
x <- paste(x, sep="", collapse = " ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary = myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq <= 15 & term.freq >=4)
dfFreq <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(dfFreq, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
m <- as.matrix(term.freq)
word.freq <- sort(rowSums(m), decreasing = T)
#word.freq <- subset(word.freq, word.freq <= 30 && word.freq >=10)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F)
library(twitteR)
api_key <- "Av24SqMdj0fi3CWaUG96qWFMu"
api_secret <- "3N2SkIWrUbqlaYqdJt4zxt3h3VStzPGp8WUO0sifx4db8LYxg7"
token <- "593955057-V5KxB4etKztnnL4W07wEj59Zhl98GOZuAwtOQlDl"
token_secret <- "qEEDFUBAgknL9oNnQNWJ6wX0ltudumThuBcbmpP6c0K8h"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
tweets <- userTimeline("KPMG", n = 100)
tweets.df <- twListToDF(tweets)
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPun <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPun))
#Add Personal Stopwords dependent on search topic
myStopwords <- c(stopwords('english'), "0", "character", "list", "contentauthor", "origin", "datetimestamp", "language", "min", "meta", "heading")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in c(1:2, 98)){
cat(paste0("[", i, "]"))
writeLines(strwrap(as.character(myCorpus[[i]]), 60))
}
stemCompletion2 <- function(x, dictionary){
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary = dictionary)
x <- paste(x, sep="", collapse = " ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary = myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq <= 15 & term.freq >=4)
dfFreq <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(dfFreq, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
m <- as.matrix(term.freq)
word.freq <- sort(rowSums(m), decreasing = T)
#word.freq <- subset(word.freq, word.freq <= 30 && word.freq >=10)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F)
library(twitteR)
api_key <- "Av24SqMdj0fi3CWaUG96qWFMu"
api_secret <- "3N2SkIWrUbqlaYqdJt4zxt3h3VStzPGp8WUO0sifx4db8LYxg7"
token <- "593955057-V5KxB4etKztnnL4W07wEj59Zhl98GOZuAwtOQlDl"
token_secret <- "qEEDFUBAgknL9oNnQNWJ6wX0ltudumThuBcbmpP6c0K8h"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
tweets <- userTimeline("Pwc Ireland", n = 100)
tweets.df <- twListToDF(tweets)
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPun <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPun))
#Add Personal Stopwords dependent on search topic
myStopwords <- c(stopwords('english'), "0", "character", "list", "contentauthor", "origin", "datetimestamp", "language", "min", "meta", "heading")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in c(1:2, 98)){
cat(paste0("[", i, "]"))
writeLines(strwrap(as.character(myCorpus[[i]]), 60))
}
stemCompletion2 <- function(x, dictionary){
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary = dictionary)
x <- paste(x, sep="", collapse = " ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary = myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq <= 15 & term.freq >=4)
dfFreq <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(dfFreq, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
m <- as.matrix(term.freq)
word.freq <- sort(rowSums(m), decreasing = T)
#word.freq <- subset(word.freq, word.freq <= 30 && word.freq >=10)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F)
library(twitteR)
api_key <- "Av24SqMdj0fi3CWaUG96qWFMu"
api_secret <- "3N2SkIWrUbqlaYqdJt4zxt3h3VStzPGp8WUO0sifx4db8LYxg7"
token <- "593955057-V5KxB4etKztnnL4W07wEj59Zhl98GOZuAwtOQlDl"
token_secret <- "qEEDFUBAgknL9oNnQNWJ6wX0ltudumThuBcbmpP6c0K8h"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
tweets <- userTimeline("Pwc Ireland", n = 100)
tweets.df <- twListToDF(tweets)
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPun <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPun))
#Add Personal Stopwords dependent on search topic
myStopwords <- c(stopwords('english'), "0", "character", "list", "contentauthor", "origin", "datetimestamp", "language", "min", "meta", "heading")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in c(1:2, 98)){
cat(paste0("[", i, "]"))
writeLines(strwrap(as.character(myCorpus[[i]]), 60))
}
stemCompletion2 <- function(x, dictionary){
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary = dictionary)
x <- paste(x, sep="", collapse = " ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary = myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq <= 15 & term.freq >=4)
dfFreq <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(dfFreq, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
m <- as.matrix(term.freq)
word.freq <- sort(rowSums(m), decreasing = T)
#word.freq <- subset(word.freq, word.freq <= 30 && word.freq >=10)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F)
endSim = 10000
#Distribution Implementation
loadingTime <- function(){
t = runif(1,min=0, max=1)
if(t<=0.3){
return(5)
}
if(t>0.3 && t<=0.8){
return(10)
}
if(t>0.8){
return(15)
}
}
weighingTime <- function(){
q = runif(1,min=0, max=1)
if(q<=0.7){
return(12)
}else{
return(16)
}
}
travelTime <- function(){
r = runif(1,min=0, max=1)
if(r<=0.4){
return(40)
}
if(r>0.4 && r<=0.7){
return(60)
}
if(r>0.7 && r<=0.9){
return(80)
}
if(r>0.9){
return(100)
}
}
#FIFO Queue Implementation <Barry Rowlingson>
#   I have utilised throughout the simulation the following implementation of
#   a FIFO queue. I have used this code in order to keep track of the specific
#   positions of each truck instead of an arbitrary queue size number. This in
#   my opinion augments the simulation. This implementation is not my work and
#   referenced as follows:
#           Barry Rowlingson, RE: Queue Implementation, Nabble R.com
#           URL: http://r.789695.n4.nabble.com/queue-implementation-td2529272.html
#           Accessed (26/03/2017)
queue = function(){
e=new.env()
q=list()
assign("q",q,envir=e)
class(e)=c("queue","environment")
e
}
push.queue=function(e,v){
q=c(v,get("q",envir=e))
assign("q",q,envir=e)
v
}
pop.queue=function(e){
q=get("q",envir=e)
v=q[[length(q)]]
if(length(q)==1){
assign("q",list(),e)
}else{
assign("q",q[1:(length(q)-1)],e)
}
return(v)
}
#Start of Simulation
rl = 1
endSim = 10000
for(i in 1:rl){
print(c("Starting sweep", i))
#Initial System States
t=0
LQ = 3    #Trucks in loading queue
WQ = 0    #Trucks in weighing queue
LT = 2    #Trucks being loaded 0,1,2
WT = 1    #Trucks being weighed 0,1
#Initial Statistic Measurements
BL = 0    #Total time on loaders
BW = 0    #Total time on scales
ND = 0    #Number of deliveries
#Queue List
LList = queue()
WList = queue()
push.queue(LList, "DT4")
push.queue(LList, "DT5")
push.queue(LList, "DT6")
#Future event list initial state
times = numeric()
times = c(loadingTime(), loadingTime(), weighingTime(), endSim)
FELS = data.frame(times = times, event = c("EL","EL", "EW", "END"), truck = c("DT1", "DT2", "DT3", "ALL"), stringsAsFactors = FALSE)
#Ststistical Measurements initial state
BL = times[1] + times[2]
BW = times[3]
event = "EL"
while(event!="END"){
#Order FEL by time
FELS = FELS[(order(as.numeric(FELS[,1]))),]
#print(FELS)
#Initial Event
t = FELS[1,1]
event = FELS[1,2]
truck = FELS[1,3]
#Remove immenent event
FELS = FELS[-c(1),]
#Result of End Loading Event
if(event=="EL"){
print(c("End Loading at time",t))
#Decrease number at loaders
LT = LT-1
#Add truck to weigh scales or queue for weigh scales
if(WT==0){
#Set end weighing future event
WT=1
t = strtoi(t, base = 0L)        #Issue with t variable being non-numeric
weighT = weighingTime()
BW = BW + weighT
tE = t + weighT
newEL = c(tE,"EW", truck)
FELS = rbind(FELS, newEL)
}else{
#Join Queue
push.queue(WList, truck)
WQ = WQ+1
}
#Add truck to loader from loading queue
if(LT<2 & LQ>0){
LQ = LQ-1
LT = LT+1
truckL = pop.queue(LList)
t = strtoi(t, base = 0L)        #Issue with t variable being non-numeric
loadingT = loadingTime()
BL = BL + loadingT
tE = t + loadingT
newEL = c(tE, "EL", truckL)
FELS = rbind(FELS, newEL)
}
}
#Result of End Weighing Event
if(event=="EW"){
print(c("End Weighing at time",t))
WT=0
#Set future arrival time event
t = strtoi(t, base = 0L)          #Issue with t variable being non-numeric
travelT = travelTime()
tE = t + travelTime()
newALQ = c(tE,"ALQ", truck)
FELS = rbind(FELS, newALQ)
#Add truck to scales from weighing queue
if(WQ>0){
WT=1
WQ = WQ-1
truckW = pop.queue(WList)
t = strtoi(t, base = 0L)        #Issue with t variable being non-numeric
weighingT = weighingTime()
tE = t + weighingT
BW = BW + weighingT
newEW = c( tE, "EW", truckW)
FELS = rbind(FELS, newEW)
}
}
#Result of Arrival Loading Queue Event
if(event=="ALQ"){
print(c("Arrive at Loading Queue at time",t))
ND = ND + 1
#Add truck to loader scales or queue for loader
if(LT<2){
LT = LT + 1
#Set end loading future event
t = strtoi(t, base = 0L)        #Issue with t variable being non-numeric
loadingT = loadingTime()
tE = t + loadingT
BL = BL + loadingT
newEL = c(tE,"EL", truck)
FELS = rbind(FELS, newEL)
}else{
#Join Queue
push.queue(LList, truck)
LQ = LQ+1
}
}
#Result of End Event
if(event=="END"){
print(c("Simulation has ended"))
print(c("The total time spent loading is ",BL))
print(c("The total time spent weighing is ",BW))
print(c("The total number of deliveries is ",ND))
}
}
}
install.packages(MASS)
library("MASS", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
mvrnorm
?mvrnorm
music = read.csv("/Users/michaeloboyle/Documents/MSISS_Year_3/MVA/Datasets/music.csv", header = T)
music = as.matrix(music)
music[1,1]
music[1,]
?eigen
A = matrix(c(1,2,2,5), nrow = 2, ncol = 2, byrow = T)
View(A)
res = eigen(A)
res
res$values[1]
res$vectors[,2]
A%*%res$vectors[,1]
A%*%res$vectors[,1]==res$values[1]*res$vectors[,1]
A%*%res$vectors[,2]==res$values[2]*res$vectors[,2]
A%*%res$vectors[,2]
res$values[2]*res$vectors[,2]
A%*%res$vectors[,2]==res$values[2]*res$vectors[,2]
musicDecomp = eigen(music)
mu = c(23, 24)
t(mu)
t(t(mu))
ev = eigen(cov)
cov= matrix(numeric(17.7, 20.3, 20.3, 24.4), nrow = 2, ncol = 2, byrow = T)
cov= matrix(c(17.7, 20.3, 20.3, 24.4), nrow = 2, ncol = 2, byrow = T)
ev = eigen(cov)
ev
?iris
install.packages(datasets)
dim iris
dim(iris)
names irir
names(iris)
fisher1 = prcomp(iris[,1:4]) #Ignore 5th categorical component
fisher1
covM = cov(iris[,1:4])
covM
eigenFisher = eigen(covM)
eigennFisher
eigenFisher
matS = eigenFisher$values^(1/2)
matS
irisCovMatrix = cov(iris[,1:4])
eigenFisher = eigen(irisCovMatrix)
stDeviations = eigenFisher$values^(1/2)
stDeviations
summary(fisher1)
plot(fisher1, type = '1')
plot(fisher1, type = lines)
?plot
plot(fisher1, type = "l")
newiris = predict(fisher1)
newiris
load("olive.Rdata")
setwd("~/Documents/GitHub/multivariate-linear-analysis")
load("~/Datasets/olive.Rdata")
load("/Users/michaeloboyle/Documents/GitHub/multivariate-linear-analysis/Datasets/olive.Rdata")
olive
